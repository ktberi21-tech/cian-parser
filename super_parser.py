"""
ğŸ•µï¸ Ğ¡Ğ£ĞŸĞ•Ğ -Ğ¡ĞšĞ Ğ«Ğ¢ĞĞ«Ğ™ ĞŸĞĞ Ğ¡Ğ•Ğ  Ğ¦Ğ˜ĞĞ
Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Excel ÑĞ¾ Ğ’Ğ¡Ğ•Ğ¥ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ‘Ğ•Ğ— Ğ‘Ğ›ĞĞšĞ˜Ğ ĞĞ’ĞšĞ˜!
"""

import asyncio
import random
import time
from pathlib import Path
from datetime import datetime
import pandas as pd
from playwright.async_api import async_playwright
import os

DOWNLOADS_DIR = Path.home() / "Downloads" / "cian_temp"
DOWNLOADS_DIR.mkdir(exist_ok=True)

class CianMegaScraper:
    def __init__(self):
        self.page = None
        self.context = None
        self.browser = None
        
    async def setup(self):
        """ğŸ”§ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ° Ñ ĞœĞĞšĞ¡Ğ˜ĞœĞĞ›Ğ¬ĞĞĞ™ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹"""
        p = await async_playwright().start()
        
        # ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ñ Ğ²Ğ°ÑˆĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹
        profile_path = str(Path.home() / ".playwright_profile")
        
        self.context = await p.chromium.launch_persistent_context(
            user_data_dir=profile_path,
            headless=False,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-first-run",
                "--no-default-browser-check",
                "--disable-popup-blocking",
                "--disable-extensions",
                "--disable-sync",
                "--disable-plugins",
                "--disable-component-update",
                "--disable-default-apps",
                "--disable-preconnect",
                "--metrics-recording-only",
                "--mute-audio",
                "--no-sandbox",
                "--disable-web-resources",
            ],
            viewport={"width": 1920, "height": 1080},
            ignore_https_errors=True,
            accept_downloads=True,
            locale="ru-RU",
            timezone_id="Europe/Moscow",
        )
        
        self.page = await self.context.new_page()
        
        # ğŸ”’ ĞœĞ°ÑĞºĞ¸Ñ€ÑƒĞµĞ¼ WebDriver
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ru-RU', 'ru', 'en-US'],
            });
            window.chrome = {
                runtime: {}
            };
        """)
        
        print("âœ… Ğ‘Ñ€Ğ°ÑƒĞ·ĞµÑ€ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹")
    
    async def human_delay(self, min_sec=1, max_sec=3):
        """â±ï¸ Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° ĞºĞ°Ğº Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°"""
        delay = random.uniform(min_sec, max_sec)
        await asyncio.sleep(delay)
    
    async def random_mouse_moves(self, count=3):
        """ğŸ–±ï¸ Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ¸"""
        for _ in range(count):
            x = random.randint(100, 1820)
            y = random.randint(100, 1000)
            await self.page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.1, 0.3))
    
    async def smooth_scroll(self):
        """ğŸ“œ ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ° ĞºĞ°Ğº Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°"""
        # ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ° Ğ²Ğ½Ğ¸Ğ·
        for _ in range(random.randint(3, 5)):
            scroll_amount = random.randint(200, 400)
            await self.page.evaluate(f"window.scrollBy(0, {scroll_amount})")
            await asyncio.sleep(random.uniform(0.3, 0.7))
        
        # Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ½Ğ¾Ğ¿ĞºĞ¸
        await self.page.evaluate("window.scrollTo(0, 0)")
        await asyncio.sleep(0.5)
    
    async def scrape_page(self, page_num, search_url):
        """ğŸ“¥ Ğ¡ĞºÑ€ĞµĞ¹Ğ¿Ğ¸Ğ½Ğ³ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹"""
        try:
            print(f"\n{'â–ˆ'*70}")
            print(f"ğŸ“„ Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦Ğ #{page_num}")
            print(f"{'â–ˆ'*70}")
            
            # 1ï¸âƒ£ ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ
            page_url = f"{search_url}&p={page_num}"
            print(f"ğŸ”— ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ğ¼: {page_url[:80]}...")
            
            await self.page.goto(page_url, wait_until="networkidle", timeout=30000)
            print(f"âœ… Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
            
            # 2ï¸âƒ£ Ğ˜Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            await self.human_delay(2, 4)
            await self.smooth_scroll()
            await self.random_mouse_moves(4)
            
            # 3ï¸âƒ£ ĞŸĞ¾Ğ¸ÑĞº ĞºĞ½Ğ¾Ğ¿ĞºĞ¸ "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» Excel"
            print("ğŸ” Ğ˜Ñ‰ĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» Excel'...")
            
            # ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ĞºĞ½Ğ¾Ğ¿ĞºĞ¸
            button_selectors = [
                'a:has-text("Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» Excel")',
                'button:has-text("Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» Excel")',
                '[class*="download"]',
                'a[href*="xlsx"]',
            ]
            
            button = None
            for selector in button_selectors:
                try:
                    button = await self.page.query_selector(selector)
                    if button:
                        print(f"âœ… ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ¿Ğ¾ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ñƒ: {selector}")
                        break
                except:
                    continue
            
            if not button:
                print("âŒ ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°! Ğ˜Ñ‰ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚...")
                # Ğ˜Ñ‰ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· JS
                button = await self.page.evaluate("""
                    () => {
                        const elements = Array.from(document.querySelectorAll('a, button, div[role="button"]'));
                        return elements.find(el => el.textContent.includes('Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» Excel'));
                    }
                """)
            
            if not button:
                print("âŒ ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ!")
                return False
            
            # 4ï¸âƒ£ Ğ¡ĞºÑ€Ğ¾Ğ»Ğ» Ğº ĞºĞ½Ğ¾Ğ¿ĞºĞµ
            await self.page.evaluate("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", button)
            await self.human_delay(1, 2)
            
            # 5ï¸âƒ£ Ğ”Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ¸ Ğº ĞºĞ½Ğ¾Ğ¿ĞºĞµ (ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº)
            await self.random_mouse_moves(3)
            
            box = await button.bounding_box()
            if box:
                # Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ¸Ğº Ğ¿Ğ¾ ĞºĞ½Ğ¾Ğ¿ĞºĞµ
                x = box['x'] + box['width'] / 2 + random.uniform(-3, 3)
                y = box['y'] + box['height'] / 2 + random.uniform(-2, 2)
                
                await self.page.mouse.move(x, y)
                await self.human_delay(0.2, 0.5)
                
                # ğŸ–±ï¸ ĞšĞ›Ğ˜Ğš!
                print("ğŸ–±ï¸ ĞĞ°Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ...")
                await self.page.mouse.click()
            
            # 6ï¸âƒ£ ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°
            print("â³ ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ° (Ğ¼Ğ°ĞºÑ 60 ÑĞµĞº)...")
            
            try:
                async with self.page.expect_download(timeout=60000) as download_info:
                    await self.human_delay(1, 3)
                
                download = await download_info.value
                file_path = DOWNLOADS_DIR / f"page_{page_num:04d}.xlsx"
                
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»
                await download.save_as(str(file_path))
                
                file_size = file_path.stat().st_size
                print(f"âœ… Ğ¤ĞĞ™Ğ› Ğ¡ĞĞ¥Ğ ĞĞĞ•Ğ!")
                print(f"   ğŸ“ {file_path.name}")
                print(f"   ğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {file_size / 1024:.1f} KB")
                
                return True
                
            except asyncio.TimeoutError:
                print("âŒ Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ñ„Ğ°Ğ¹Ğ»Ğ°!")
                return False
                
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ {page_num}: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            # 7ï¸âƒ£ ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ: Ğ–Ğ´ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†ĞµĞ¹
            # Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° (ĞĞ§Ğ•ĞĞ¬ Ğ’ĞĞ–ĞĞ!)
            delay = random.uniform(8, 20)
            print(f"â³ Ğ–Ğ´ĞµĞ¼ {delay:.1f} ÑĞµĞº Ğ¿ĞµÑ€ĞµĞ´ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†ĞµĞ¹...")
            print(f"   (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¦Ğ¸Ğ°Ğ½ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑĞ», Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚)")
            await self.human_delay(delay - 3, delay)
    
    async def merge_excel_files(self):
        """ğŸ“Š ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… Excel Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ¸Ğ½"""
        print(f"\n{'='*70}")
        print("ğŸ”— ĞĞ‘ĞªĞ•Ğ”Ğ˜ĞĞ¯Ğ•Ğœ Ğ’Ğ¡Ğ• Ğ¤ĞĞ™Ğ›Ğ« Ğ’ ĞĞ”Ğ˜Ğ!")
        print(f"{'='*70}")
        
        xlsx_files = sorted(DOWNLOADS_DIR.glob("page_*.xlsx"))
        
        if not xlsx_files:
            print("âŒ Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹!")
            return None
        
        print(f"ğŸ“ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(xlsx_files)}")
        
        all_data = []
        
        for file_path in xlsx_files:
            try:
                print(f"   ğŸ“– Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼: {file_path.name}...", end=" ")
                df = pd.read_excel(file_path)
                all_data.append(df)
                print(f"âœ… ({len(df)} ÑÑ‚Ñ€Ğ¾Ğº)")
            except Exception as e:
                print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
                continue
        
        if not all_data:
            print("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ Ñ„Ğ°Ğ¹Ğ»!")
            return None
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ²ÑĞµ
        print(f"\nğŸ”€ ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼...")
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
        print(f"ğŸ” Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹...")
        original_count = len(combined_df)
        combined_df = combined_df.drop_duplicates()
        duplicates_count = original_count - len(combined_df)
        
        print(f"   â€¢ Ğ‘Ñ‹Ğ»Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº: {original_count}")
        print(f"   â€¢ Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²: {duplicates_count}")
        print(f"   â€¢ ĞÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ: {len(combined_df)}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = Path.home() / "Desktop" / f"cian_combined_{timestamp}.xlsx"
        
        print(f"\nğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼: {output_file.name}...")
        combined_df.to_excel(output_file, index=False)
        
        print(f"âœ… Ğ“ĞĞ¢ĞĞ’Ğ!")
        print(f"   ğŸ“Š {output_file}")
        print(f"   ğŸ“ˆ Ğ’ÑĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹: {len(combined_df)}")
        print(f"   ğŸ“ Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {output_file.stat().st_size / (1024*1024):.1f} MB")
        
        return output_file
    
    async def run(self, search_url, max_pages):
        """ğŸš€ Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ»"""
        await self.setup()
        
        successful = 0
        failed = 0
        
        for page_num in range(1, max_pages + 1):
            success = await self.scrape_page(page_num, search_url)
            
            if success:
                successful += 1
            else:
                failed += 1
                print("âš ï¸ ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· 20 ÑĞµĞº...")
                await self.human_delay(15, 25)
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ¯")
        print(f"{'='*70}")
        print(f"âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ¾: {successful} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†")
        print(f"âŒ ĞÑˆĞ¸Ğ±Ğ¾Ğº: {failed}")
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹
        await self.merge_excel_files()
        
        await self.context.close()

async def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         ğŸ•µï¸  ĞœĞ•Ğ“Ğ-Ğ¡ĞšĞ Ğ«Ğ¢ĞĞ«Ğ™ ĞŸĞĞ Ğ¡Ğ•Ğ  Ğ¦Ğ˜ĞĞ (Excel ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ)        â•‘")
    print("â•‘         Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ±ĞµĞ· Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ!                â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # Ğ’Ğ°ÑˆĞ° ÑÑÑ‹Ğ»ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (Ğ±ĞµĞ· ?p=N)
    search_url = input("ğŸ“Œ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ URL Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¦Ğ˜ĞĞ (Ğ±ĞµĞ· ?p=): ").strip()
    if not search_url:
        search_url = "https://www.cian.ru/sale/flat/?bez_apartamentov=1&room1=1&room2=1&room3=1&room4=1&room5=1"
    
    max_pages = int(input("ğŸ“ Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞºÑ€ĞµĞ¹Ğ¿Ğ¸Ñ‚ÑŒ? (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ 2-5 Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°): ").strip() or "3")
    
    scraper = CianMegaScraper()
    await scraper.run(search_url, max_pages)

if __name__ == "__main__":
    asyncio.run(main())
